

## Machine Learning Assignment: Activity Tracking
## By Rachel, 22 Mar 2015

## Overview of analysis process
1. Setup
2. Data download and cleaning  
3. Creating data sets
4. Analysis 
5. Validation
6. Fine Tuning
7. Submission

## Step 1 Setup


```r
library(ggplot2)
library(caret)
library(randomForest)
library(e1071)
library(gbm)
library(doParallel)
library(rattle)
library(survival)
library(splines)
library(plyr)
library(tree)
```

## Step 2: Data download and cleaning
After downloading data, clean data by removing“#DIV/0!”, replacing with NA value.


```r
training <- read.csv("pml-training.csv", na.strings=c("#DIV/0!"), row.names = 1)
testing <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!"), row.names = 1)

training <- training[, 6:dim(training)[2]]

threshold <- dim(training)[1] * 0.95
# Remove columns too many NA values

goodColumns <- !apply(training, 2, function(x) sum(is.na(x)) > threshold  || sum(x=="") > threshold)

training <- training[, goodColumns]

badColumns <- nearZeroVar(training, saveMetrics = TRUE)

training <- training[, badColumns$nzv==FALSE]

training$classe = factor(training$classe)
```

## Step 3: Creating data sets

Create partitioning in rows for training and validation


```r
inTrain <- createDataPartition(training$classe, p = 0.6)[[1]]
crossv <- training[-inTrain,]
training <- training[ inTrain,]
inTrain <- createDataPartition(crossv$classe, p = 0.75)[[1]]
crossv_test <- crossv[ -inTrain,]
crossv <- crossv[inTrain,]

testing <- testing[, 6:dim(testing)[2]]
testing <- testing[, goodColumns]
testing$classe <- NA
testing <- testing[, badColumns$nzv==FALSE]
```
## Step 4: Analysis 
Analysis is done using tree method


```r
set.seed(12345)
tree.training=tree(classe~.,data=training)
summary(tree.training)
```

```
## 
## Classification tree:
## tree(formula = classe ~ ., data = training)
## Variables actually used in tree construction:
##  [1] "roll_belt"         "pitch_forearm"     "roll_forearm"     
##  [4] "magnet_dumbbell_x" "num_window"        "magnet_dumbbell_y"
##  [7] "magnet_dumbbell_z" "pitch_belt"        "yaw_belt"         
## [10] "roll_dumbbell"     "accel_forearm_x"  
## Number of terminal nodes:  21 
## Residual mean deviance:  1.426 = 16760 / 11760 
## Misclassification error rate: 0.2779 = 3272 / 11776
```

```r
plot(tree.training)
text(tree.training,pretty=0, cex = 0.8)
```

![plot of chunk unnamed-chunk-4](figure/unnamed-chunk-4-1.png) 
Next step is to prune the tree

```r
modFit <- train(classe ~ .,method="rpart",data=training)
print(modFit$finalModel)
```

```
## n= 11776 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 11776 8428 A (0.28 0.19 0.17 0.16 0.18)  
##    2) roll_belt< 130.5 10768 7429 A (0.31 0.21 0.19 0.18 0.11)  
##      4) pitch_forearm< -34 956    5 A (0.99 0.0052 0 0 0) *
##      5) pitch_forearm>=-34 9812 7424 A (0.24 0.23 0.21 0.2 0.12)  
##       10) yaw_belt>=169.5 484   46 A (0.9 0.039 0 0.052 0.0041) *
##       11) yaw_belt< 169.5 9328 7073 B (0.21 0.24 0.22 0.2 0.12)  
##         22) magnet_dumbbell_z< -87.5 1265  544 A (0.57 0.29 0.045 0.074 0.025) *
##         23) magnet_dumbbell_z>=-87.5 8063 6066 C (0.15 0.23 0.25 0.22 0.14)  
##           46) num_window< 241.5 1764 1123 A (0.36 0.05 0.096 0.25 0.24) *
##           47) num_window>=241.5 6299 4472 C (0.093 0.29 0.29 0.22 0.11)  
##             94) magnet_dumbbell_x>=-445.5 2145 1114 B (0.062 0.48 0.076 0.21 0.17) *
##             95) magnet_dumbbell_x< -445.5 4154 2489 C (0.11 0.19 0.4 0.22 0.08) *
##    3) roll_belt>=130.5 1008    9 E (0.0089 0 0 0 0.99) *
```

```r
fancyRpartPlot(modFit$finalModel)
```

![plot of chunk unnamed-chunk-5](figure/unnamed-chunk-5-1.png) 

The data shows that the 'caret' package is close to the 'tree' package

As the tree has too many variables, pruning is applied using cross validation


```r
cv.training=cv.tree(tree.training,FUN=prune.misclass)
cv.training
```

```
## $size
##  [1] 21 20 19 18 17 16 15 14 13 11 10  9  6  5  4  2  1
## 
## $dev
##  [1] 3119 3119 3140 3370 3435 3536 3625 4033 4071 4473 4565 4854 6427 6971
## [15] 7132 7438 8428
## 
## $k
##  [1]     -Inf   0.0000  56.0000  90.0000  97.0000 104.0000 115.0000
##  [8] 138.0000 146.0000 197.5000 229.0000 269.0000 333.3333 359.0000
## [15] 363.0000 402.5000 990.0000
## 
## $method
## [1] "misclass"
## 
## attr(,"class")
## [1] "prune"         "tree.sequence"
```

```r
plot(cv.training)
```

![plot of chunk unnamed-chunk-6](figure/unnamed-chunk-6-1.png) 

```r
prune.training=prune.misclass(tree.training,best=18)
```
## 5. Validation
Carrying out analysis on pruned data tree

```r
tree.pred=predict(prune.training,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix))
```

```
## [1] NaN
```

As the error rate is 0.66, pruning does not affect the accuracy to a large degree.
However as a single tree is insufficient, bootstrapping will be used to improve accuracy via random forest 

## 6. Fine Tuning
# Random forests

```r
set.seed(12345)
rf.training=randomForest(classe~.,data=training,ntree=100, importance=TRUE)
rf.training
```

```
## 
## Call:
##  randomForest(formula = classe ~ ., data = training, ntree = 100,      importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 100
## No. of variables tried at each split: 7
## 
##         OOB estimate of  error rate: 0.48%
## Confusion matrix:
##      A    B    C    D    E  class.error
## A 3346    2    0    0    0 0.0005973716
## B    8 2266    5    0    0 0.0057042563
## C    0   14 2039    1    0 0.0073028238
## D    0    0   16 1913    1 0.0088082902
## E    0    0    0    9 2156 0.0041570439
```

```r
varImpPlot(rf.training,)
```

![plot of chunk unnamed-chunk-8](figure/unnamed-chunk-8-1.png) 

From this graph it can see seen which variables have the higher impact on prediction
## 7. Submission
Here we predict the testing data


```r
tree.pred=predict(rf.training,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
```
This shows that the random forest method is succesful.

# Preparing data for submission


```r
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
x <- testing

answers <- predict(rf.training, newdata=x)
answers
```

```
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
## Levels: A B C D E
```

```r
pml_write_files(answers)
```
